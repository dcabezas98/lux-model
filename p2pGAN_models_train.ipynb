{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbpm-9eV0RZt"
   },
   "outputs": [],
   "source": [
    "# For Google Colab to access Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# David Cabezas Berrido\n",
    "\n",
    "# p2pGAN_models-train.ipynb\n",
    "\n",
    "# Definition of the models (generator and discriminator) and training\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "PATH = '.' #'/content/drive/My Drive/TFG-Image-Optimization' # for Google Colab\n",
    "INPATH = PATH+'/data/jpeg/short/'\n",
    "TGPATH = PATH+'/data/jpeg/long/'\n",
    "MODELS = PATH+'/models/' # for the trained models to be saved\n",
    "\n",
    "# Dimensions for training images\n",
    "TRAIN_HEIGHT = 1024 \n",
    "TRAIN_WIDTH = 1536\n",
    "TRAIN_SQUARE_SIZE = 512 # Length of the random cropped square side\n",
    "\n",
    "# Dimensions for test images\n",
    "TEST_HEIGHT = 3072 #=12*256 # This resolution requires a large RAM\n",
    "TEST_WIDTH = 4608 #=18*256\n",
    "\n",
    "CHANNELS = 3 # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azHIRZoHQi8H"
   },
   "outputs": [],
   "source": [
    "# Dataset reading\n",
    "\n",
    "imgurls = !ls -1 \"{INPATH}\" # short exposure images\n",
    "\n",
    "# Match each short exposure image with the corresponding long exposure image\n",
    "# Example:\n",
    "# Short exposure: fuji-00001-x4.jpg\n",
    "# Long exposure: fuji-00001.jpg\n",
    "def urlTarget(url):\n",
    "    ext=url.split('.')[-1]\n",
    "    u=url.split('-')\n",
    "    return ''.join([u[0],'-',u[1],'.'+ext])\n",
    "\n",
    "imgurls_tg=[(url,urlTarget(url)) for url in imgurls]\n",
    "\n",
    "# Train/Test split\n",
    "def isForTrain(url):\n",
    "    u=url.split('-')\n",
    "    return u[1][0] in {'0','1'}\n",
    "\n",
    "# Train images\n",
    "tr_urls = [url for url in imgurls_tg if isForTrain(url[0])] # First digit is 0 or 1\n",
    "# Test images\n",
    "ts_urls = [url for url in imgurls_tg if not isForTrain(url[0])] # First digit is 2\n",
    "\n",
    "np.random.shuffle(tr_urls)\n",
    "#np.random.shuffle(ts_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7kKz3RZQi8u"
   },
   "outputs": [],
   "source": [
    "# Dataset processing\n",
    "\n",
    "# Resizes image to height x width\n",
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return input_image, real_image\n",
    "\n",
    "# Randomly crops a height x width square\n",
    "def random_crop(input_image, real_image, height, width):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, size=[2, height, width, 3])\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "# Randomly mirrors (flips horizontally) both images\n",
    "def random_flip(input_image, real_image):\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "    return input_image, real_image\n",
    "\n",
    "# Normalize to [-1,1]\n",
    "def normalize(inimg,tgimg):\n",
    "    inimg = inimg/127.5-1\n",
    "    tgimg = tgimg/127.5-1\n",
    "    return inimg, tgimg\n",
    "\n",
    "@tf.function\n",
    "def loadTrainImage(filenames):\n",
    "    # Read image\n",
    "    inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(INPATH+filenames[0])),tf.float32)[...,:3]\n",
    "    tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(TGPATH+filenames[1])),tf.float32)[...,:3]\n",
    "    inimg, tgimg = resize(inimg, tgimg, TRAIN_HEIGHT, TRAIN_WIDTH)\n",
    "    inimg, tgimg = random_crop(inimg, tgimg, TRAIN_SQUARE_SIZE, TRAIN_SQUARE_SIZE)\n",
    "    inimg, tgimg = random_flip(inimg, tgimg)\n",
    "    inimg, tgimg = normalize(inimg, tgimg)\n",
    "    return inimg, tgimg\n",
    "    \n",
    "def loadTestImage(filenames):\n",
    "    # Read image\n",
    "    inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(INPATH+filenames[0])),tf.float32)[...,:3]\n",
    "    tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(TGPATH+filenames[1])),tf.float32)[...,:3]\n",
    "    inimg, tgimg = resize(inimg, tgimg, TEST_HEIGHT, TEST_WIDTH)\n",
    "    inimg, tgimg = normalize(inimg, tgimg)\n",
    "    return inimg, tgimg\n",
    "    \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\n",
    "train_dataset = train_dataset.map(loadTrainImage, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(ts_urls)\n",
    "test_dataset = test_dataset.map(loadTestImage, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vglc7Q3mQi9X"
   },
   "outputs": [],
   "source": [
    "# Block of 2-3 layers, appears in discriminator and first half of generator (encoder)\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # Convolution layer\n",
    "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "               kernel_initializer=initializer, use_bias=not apply_batchnorm))\n",
    "    if apply_batchnorm: # Batch normalization layer (if indicated)\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(tf.keras.layers.LeakyReLU()) # Leaky ReLU layer\n",
    "    return result\n",
    "\n",
    "# Block of 3-4 layers, appear in second half of generator (decoder)\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    # Deconvolution layer\n",
    "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "               padding='same', kernel_initializer=initializer, use_bias=False))\n",
    "    result.add(tf.keras.layers.BatchNormalization()) # Batch normalization layer\n",
    "    if apply_dropout: # Dropout layer (if indicated)\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    result.add(tf.keras.layers.ReLU()) # ReLU layer\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diuaVvVPQi9k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The generator use U-Net architecture: enconder-decoder with skip connections\n",
    "\n",
    "def Generator(): \n",
    "    inputs = tf.keras.layers.Input(shape=[None,None,CHANNELS]) # Shape (if input has 512x512x3)\n",
    "    #inputs = tf.keras.layers.Input(shape=[512,512,CHANNELS]) # Shape (if input has 512x512x3)\n",
    "    \n",
    "    down_stack = [ # Encoder                      \n",
    "        downsample(64, 4, apply_batchnorm=False), # (bs, 256, 256, 64)\n",
    "        downsample(128, 4),                       # (bs, 128, 128, 128)\n",
    "        downsample(256, 4),                       # (bs, 64, 64, 256)\n",
    "        downsample(512, 4),                       # (bs, 32, 32, 512)\n",
    "        downsample(512, 4),                       # (bs, 16, 16, 512)\n",
    "        downsample(512, 4),                       # (bs, 8, 8, 512)\n",
    "        downsample(512, 4),                       # (bs, 4, 4, 512)\n",
    "        downsample(512, 4),                       # (bs, 2, 2, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [ # Decoder\n",
    "        upsample(512, 4, apply_dropout=True),     # (bs, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),     # (bs, 8, 8, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),     # (bs, 16, 16, 1024)\n",
    "        upsample(512, 4),                         # (bs, 32, 32, 1024)\n",
    "        upsample(256, 4),                         # (bs, 64, 64, 512)\n",
    "        upsample(128, 4),                         # (bs, 128, 128, 256)\n",
    "        upsample(64, 4),                          # (bs, 256, 256, 128)\n",
    "    ]\n",
    "\n",
    "    # Final layer: must ouput same shape (batch size, height, width, channels) as input\n",
    "    # Pixel values must be in [-1,1] so activation=tanh\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(filters=CHANNELS, kernel_size=4,\n",
    "                                           strides=2, padding='same',\n",
    "                                           kernel_initializer=initializer,\n",
    "                                           activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "    # Downsampling through the model (encoder)\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    # Upsampling and establishing the skip connections (decoder)\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator=Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generator architecture (for Jupyter notebook)\n",
    "#tf.keras.utils.plot_model(generator, show_shapes=True, dpi=96)\n",
    "#tf.keras.utils.plot_model(generator, show_shapes=False, dpi=96)\n",
    "#tf.keras.utils.plot_model(generator, show_shapes=False, expand_nested=True, dpi=96)\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, expand_nested=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwAteD9aQi9x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PatchGAN:\n",
    "# Outputs NxN 1-channel image where each pixel classifies a 70x70 portion of input\n",
    "def Discriminator():\n",
    "    inp = tf.keras.layers.Input(shape=[None, None, CHANNELS], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[None, None, CHANNELS], name='target_image')\n",
    "    #inp = tf.keras.layers.Input(shape=[512, 512, CHANNELS], name='input_image')\n",
    "    #tar = tf.keras.layers.Input(shape=[512, 512, CHANNELS], name='target_image')\n",
    "    \n",
    "    # Shape (if input has 512x512x3)\n",
    "    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 512, 512, CHANNELS*2=6)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x) # (bs, 256, 256, 64)\n",
    "    down2 = downsample(128, 4)(down1) # (bs, 128, 128, 128)\n",
    "    down3 = downsample(256, 4)(down2) # (bs, 64, 64, 256)\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 66, 66, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) # (bs, 63, 63, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv) # (bs, 63, 63, 512)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1) # (bs, 63, 63, 512)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 65, 65, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "            kernel_initializer=initializer)(zero_pad2) # (bs, 62, 62, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "discriminator=Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot discriminator architecture (for Jupyter notebook)\n",
    "#tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=96)\n",
    "#tf.keras.utils.plot_model(discriminator, show_shapes=False, dpi=96)\n",
    "#tf.keras.utils.plot_model(discriminator, show_shapes=False, expand_nested=True, dpi=96)\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, expand_nested=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-TEoynGQi9r"
   },
   "outputs": [],
   "source": [
    "# Cross-entropy between true and prediction\n",
    "# from_logits forces the prediction to be between [0,1] (applies sigmoid to the prediction)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "LAMBDA = 100 # Weight of L1 loss\n",
    "# Loss function for generator\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    # Generator tries to fool discriminator\n",
    "    # Discriminator is fooled if disc_generated_output has low values\n",
    "    # (discriminator thinks the generated image is real)\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    # Mean Absolute Error: prediction and ground truth should look alike\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    total_gen_loss = gan_loss + LAMBDA * l1_loss\n",
    "    return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "# Loss function for discriminator\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    # Discriminator must know that ground truth is real, should output low values\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    # Discriminator must know that generated image is false, should output high values\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    return real_loss + generated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be1pWewoQi-B"
   },
   "outputs": [],
   "source": [
    "# Optimizers: Adam algorithm with beta_1=0.5, beta_2=0.999 (default)\n",
    "# the rest of parameters are set to default\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouuHJRSfQi-F"
   },
   "outputs": [],
   "source": [
    "# Code for checkpoint saving, to restore the status of both models\n",
    "# and their respective optimizers\n",
    "checkpoint_dir = PATH+'/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "# Code for restoring a specific checkpoint or the most recent\n",
    "#checkpoint.restore(checkpoint_prefix+'-18').assert_consumed()\n",
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfIl99BEQi-V"
   },
   "outputs": [],
   "source": [
    "# A single train step through one example\n",
    "@tf.function\n",
    "def train_step(input_image, target):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True) # Generated image\n",
    "        # Discriminator tries to guess whether the images are real or generated\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "        # Loss of each model\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        # Gradients are computed and weights updated\n",
    "        generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnJ-pQ8qQi-I"
   },
   "outputs": [],
   "source": [
    "# Plot input-target-prediction\n",
    "# to observe the generator performance or it's evolution throughout training\n",
    "def generate_images(model, test_input, tar):\n",
    "    # training=True so batchnorm use test data instead of moving mean and variance from train data\n",
    "    # Batch size must be 1 so prediction does not depend in other test examples\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWOFREMqQi-a"
   },
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds):\n",
    "    for epoch in range(epochs):\n",
    "        # generator evolution throughout training\n",
    "        for example_input, example_target in test_ds.take(1):\n",
    "            generate_images(generator, example_input, example_target)\n",
    "        print(\"Epoch: \", epoch)\n",
    "        # Train: iterate through all training examples every single epoch\n",
    "        for n, (input_image, target) in train_ds.enumerate():\n",
    "            if (n+1) % 50 == 0: # Number of examples in the current epoch\n",
    "                print(int(n+1),end=', ')\n",
    "                if (n+1) % 1000 == 0:\n",
    "                    print()\n",
    "            train_step(input_image, target)\n",
    "        print()\n",
    "        # saving (checkpoint) the model every 5 epochs\n",
    "        if (epoch+1) % 5 == 0 and epoch+1 < epochs: # Not the last one\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "    checkpoint.save(file_prefix = checkpoint_prefix) # Saving when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2D-ycGeQi-f"
   },
   "outputs": [],
   "source": [
    "# 5 epochs of training\n",
    "# The total number of epochs was 75\n",
    "fit(train_dataset, 5, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models: SavedModel format\n",
    "generator.save(MODELS+'GAN-generator')\n",
    "discriminator.save(MODELS+'GAN-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models: HDF5 format\n",
    "generator.save(MODELS+'GAN-generator.h5')\n",
    "discriminator.save(MODELS+'GAN-discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50zegsBpqqwP"
   },
   "outputs": [],
   "source": [
    "# Export generator directly to JavaScript\n",
    "# this can also be done from the terminal with:\n",
    "\"\"\"\n",
    "tensorflowjs_converter --input_format keras path/to/GAN-generator.h5 path/to/TFJS_GAN-generator\n",
    "\"\"\"\n",
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(generator, MODELS+'TFJS_GAN-generator')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p2pGAN_models-train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
